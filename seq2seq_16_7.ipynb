{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import sys\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import Accuracy\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "import time\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocab class for input and output language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    \"\"\"\n",
    "    Lang(self,name:str)\n",
    "    Attributes:\n",
    "        word2index {dict}: Dictionary for mapping word to vocab indexes\n",
    "        index2word {dict}: Dictionary for mapping vocab index to words\n",
    "        word2count {count}: Dictionary for mapping word to its frequency of appearance in dataset\n",
    "    Methods:\n",
    "        addSentence:\n",
    "            args:\n",
    "                sentence {str}\n",
    "            Adds words from sentence to vocab \n",
    "        addWord\n",
    "    \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read languages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Language instances for vocabulary utilizations\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 30\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p,reverse):\n",
    "    if reverse is not True:\n",
    "        return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "            len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "            p[0].startswith(eng_prefixes)\n",
    "    else:\n",
    "        return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "            len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "            p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs,reverse):\n",
    "    return [pair for pair in pairs if filterPair(pair,reverse)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([1,2,3]).tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess all text from txt -> Create pairs from entire data and add words from data to vocab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 5173\n",
      "eng 3388\n",
      "['je suis completement confuse', 'i am totally confused']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False,test_size=None):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs,reverse)\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True,1000)\n",
    "print(random.choice(pairs))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize data, create tensors of the data and create dataset and data loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=30\n",
    "########################################################################################################\n",
    "  \n",
    "def indexesFromSentence(lang,sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang,sentence):\n",
    "    indexes=indexesFromSentence(lang,sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long,device=device).view(1,-1)\n",
    "\n",
    "def tensorFromPair(pair):\n",
    "    input_tensor=tensorFromSentence(input_lang,pair[0])\n",
    "    output_tensor=tensorFromSentence(output_lang,pair[1])\n",
    "    return(input_tensor,output_tensor)\n",
    "\n",
    "\n",
    "####################################################################################################################################################\n",
    "\n",
    "\n",
    "def get_dataloader(lang1,lang2,batch_size,test_ratio=0.1):\n",
    "    input_lang,output_lang,pairs=prepareData(lang1,lang2,True)\n",
    "    N=len(pairs)\n",
    "    input_ids=torch.zeros(size=(N,MAX_LENGTH),dtype=torch.long)\n",
    "    output_ids=torch.zeros(size=(N,MAX_LENGTH),dtype=torch.long)\n",
    "\n",
    "    for idx,(inp,trg) in enumerate(pairs):\n",
    "        try:\n",
    "            input_tensor,output_tensor=tensorFromPair((inp,trg))\n",
    "        except KeyError:\n",
    "            print(f\"error at {idx}th pair\")\n",
    "            print(inp,trg)\n",
    "            continue\n",
    "        input_ids[idx,:input_tensor.shape[1]]=input_tensor\n",
    "        output_ids[idx,:output_tensor.shape[1]]=output_tensor\n",
    "    \n",
    "    test_size=int(N*test_ratio)\n",
    "        \n",
    "    test_idx=np.random.randint(low=0,high=N,size=test_size)\n",
    "    train_idx=np.setdiff1d(np.arange(N),test_idx)\n",
    "    \n",
    "    train_inp,train_outp=input_ids[train_idx],output_ids[train_idx]\n",
    "    test_inp,test_outp=input_ids[test_idx],output_ids[test_idx]\n",
    "    \n",
    "    train_data=TensorDataset(torch.LongTensor(train_inp).to(device),\n",
    "                             torch.LongTensor(train_outp).to(device)\n",
    "                             )\n",
    "    test_data=TensorDataset(torch.LongTensor(test_inp).to(device),\n",
    "                             torch.LongTensor(test_outp).to(device)\n",
    "                             )\n",
    "    \n",
    "    train_sampler=RandomSampler(train_data)\n",
    "    train_dataloader=DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
    "\n",
    "    test_sampler=RandomSampler(test_data)\n",
    "    test_dataloader=DataLoader(test_data,sampler=test_sampler,batch_size=batch_size)\n",
    "    \n",
    "    return input_lang,output_lang,train_dataloader,test_dataloader\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL BUILDING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self,input_dim,heads):\n",
    "        super().__init__()\n",
    "        head_dim=input_dim//heads\n",
    "        self.heads=heads\n",
    "        self.head_dim=head_dim\n",
    "        assert input_dim%heads==0\n",
    "\n",
    "        self.query=nn.Linear(head_dim,head_dim)\n",
    "        self.key=nn.Linear(head_dim,head_dim)\n",
    "        self.value=nn.Linear(head_dim,head_dim)\n",
    "        self.fc_out=nn.Linear(input_dim,input_dim)\n",
    "    \n",
    "    def forward(self,queries,keys,values):\n",
    "        N=queries.shape[0]\n",
    "        ql,kl,vl=queries.shape[1],keys.shape[1],values.shape[1]   #Q,K,V\n",
    "        queries=torch.transpose(queries.reshape(N,ql,self.heads,self.head_dim),1,2) #NHQD\n",
    "        keys=torch.transpose(keys.reshape(N,kl,self.heads,self.head_dim),1,2)   #NHKD\n",
    "        values=torch.transpose(keys.reshape(N,vl,self.heads,self.head_dim),1,2)   #NHVD   \n",
    "        queries_=self.query(queries)\n",
    "        keys_=self.key(keys)\n",
    "        values_=self.value(values)\n",
    "\n",
    "        attention=torch.bmm(queries_,torch.transpose(keys_-1,-2))           #NHQD . NHDK -> NHQK\n",
    "        \n",
    "        attention_norm=torch.softmax(attention/(self.head_dim**0.5),axis=3)\n",
    "        attention_out=torch.transpose(torch.bmm(attention_norm,values_),1,2)    #NHQK . NHVD -> NHQD (V=K)\n",
    "        out=self.fc_out(attention_out.reshape(N,ql,self.heads*self.head_dim))\n",
    "        return out \n",
    "        \n",
    "\n",
    "# class TransformerBlock(nn.Module):\n",
    "#     def __init__(self,input_dim,heads,expansion,dropout):\n",
    "#         self.expansion=input_dim*expansion\n",
    "#         self.attention=SelfAttention(input_dim,heads)\n",
    "#         self.dropout=nn.Dropout(dropout)\n",
    "#         self.norm1=nn.LayerNorm(input_dim)\n",
    "#         self.norm2=nn.LayerNorm(input_dim)\n",
    "#         self.expansion_layer=nn.Sequential(\n",
    "#             nn.Linear(input_dim,self.expansion),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(expansion,input_dim)\n",
    "#         )   \n",
    "\n",
    "#     def forward(self,query,key,value,mask):\n",
    "#         attention=self.attention(query,key,value)\n",
    "\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,dropout_p=0.1,vocab_size=None):\n",
    "        super(EncoderRNN,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding=nn.Embedding(vocab_size,hidden_size)\n",
    "        self.gru=nn.GRU(hidden_size,hidden_size,batch_first=True)\n",
    "        self.dropout=nn.Dropout(dropout_p)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        embedded=self.dropout(self.embedding(input))\n",
    "        out,hidden_state=self.gru(embedded)\n",
    "        return out,hidden_state\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,output_size,hidden_size,vocab_size=None):\n",
    "        super(DecoderRNN,self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab_size,hidden_size)\n",
    "        self.gru=nn.GRU(hidden_size,hidden_size,batch_first=True)\n",
    "        self.out=nn.Linear(hidden_size,output_size)\n",
    "    \n",
    "    def forward_step(self,input,hidden):\n",
    "        embedded=self.embedding(input)\n",
    "        output,hidden=self.gru(embedded,hidden)\n",
    "        output=self.out(output)\n",
    "        return output,hidden \n",
    "\n",
    "    def forward(self,encoder_outputs,encoder_hidden,target_tensor=None):\n",
    "        N=encoder_outputs.size(0)\n",
    "        decoder_input=torch.empty(N,1,dtype=torch.long,device=device).fill_(SOS_token)\n",
    "        decoder_hidden=encoder_hidden\n",
    "        decoder_outputs=[]\n",
    "        for i in range(MAX_LENGTH):\n",
    "            gru_out,decoder_hidden=self.forward_step(decoder_input,decoder_hidden)\n",
    "            decoder_outputs.append(gru_out)\n",
    "            if target_tensor is not None:\n",
    "                decoder_input=target_tensor[:,i].unsqueeze(1)\n",
    "\n",
    "            else:\n",
    "                _,topi=gru_out.topk(1)\n",
    "                decoder_input=topi.squeeze(-1).detach()\n",
    "\n",
    "        \n",
    "        decoder_outputs=torch.cat(decoder_outputs,dim=1)\n",
    "        decoder_outputs=F.log_softmax(decoder_outputs,dim=-1)\n",
    "        return decoder_outputs,decoder_hidden,None\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bahdanau Attention Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBiRNN(nn.Module):\n",
    "    def __init__(self,vocab_size,num_layers,input_size,hidden_size):\n",
    "        super(EncoderBiRNN,self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab_size,input_size)\n",
    "        self.BiRNN=nn.GRU(input_size,hidden_size, num_layers=num_layers,bidirectional=True,batch_first=True)\n",
    "        \n",
    "    def forward(self,x,hidden=None):\n",
    "        N,max_L=x.shape\n",
    "        hiddens=[]\n",
    "        outputs=[]\n",
    "        for i in range(max_L):\n",
    "            embedding=self.embedding(x[:,i].unsqueeze(1))\n",
    "            output,hidden=self.BiRNN(embedding,hidden)\n",
    "            outputs.append(output)\n",
    "            hiddens.append(hidden.unsqueeze(0))\n",
    "        outputs=torch.cat(outputs,dim=1)            # outputs [max_L,N,1,enc_hidden_size]\n",
    "        hiddens=torch.cat(hiddens,dim=0)            # hiddens [max_L,enc_nlayers*directions,N,enc_hidden_size]\n",
    "\n",
    "        return outputs,hiddens\n",
    "    \n",
    "    #encoding_dict={directions=enc_directions,\n",
    "        # enc_hidden_size=enc_hidden_size,\n",
    "        # dec_hidden_size=enc_hidden_size,\n",
    "        # enc_num_layers=enc_num_layers,\n",
    "        # dec_num_layers=dec_num_layers}\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self,encoding_dict):\n",
    "        super(BahdanauAttention,self).__init__()\n",
    "        self.encoding_dict=encoding_dict\n",
    "        self.Wa=nn.Linear(self.encoding_dict['dec_hidden_size']*self.encoding_dict['dec_num_layers'],self.encoding_dict['dec_hidden_size'])\n",
    "        self.Ua=nn.Linear(self.encoding_dict[\"enc_hidden_size\"],self.encoding_dict['dec_hidden_size'])\n",
    "        self.Va=nn.Linear(self.encoding_dict[\"dec_hidden_size\"],1)\n",
    "\n",
    "    def forward(self,query,keys): \n",
    "        #query [dec_num_layers,N,dec_hidden]; keys  [max_L,(enc_nlayers*directions),N,enc_hidden_size]  \n",
    "        \n",
    "        #  [dec_num_layers,N,dec_hidden]-> [N,dec_num_layers,dec_hidden]-> [N,1,dec_num_layers*dec_hidden]\n",
    "        query=query.transpose(0,1)\n",
    "        query=query.reshape(query.size(0),1,query.size(1)*query.size(2))    \n",
    "        #  [max_L,(enc_nlayers*directions),N,enc_hidden_size] -> [max_L*(enc_nlayers*directions),N,enc_hidden_size] -> [N,max_L*(enc_nlayers*directions),enc_hidden_size]\n",
    "        keys=keys.reshape(-1,keys.size(2),keys.size(3)).transpose(0,1)\n",
    "        \n",
    "        #  query: [N, 1, dec_num_layers*dec_hidden] @ [dec_hidden*dec_num_layers, dec_hidden] -> [N, 1, dec_hidden]\n",
    "        #  keys:  [N, max_L*(enc_nlayers*directions), enc_hidden_size] @ [enc_hidden_size, dec_hidden] -> [N, max_L*(enc_nlayers*directions), dec_hidden]\n",
    "        \n",
    "        #  score(addition): [N, max_L*(enc_nlayers*directions), dec_hidden] + [N, 1, dec_hidden](broadcasted) -> [N, max_L*(enc_nlayers*directions), dec_hidden]\n",
    "        #  score(reduce_sum ): [N, max_L*(enc_nlayers*directions), dec_hidden] @ [dec_hidden, 1] -> [N, max_L*(enc_nlayers*directions), 1]                \n",
    "        scores=self.Va(torch.tanh(self.Wa(query)+self.Ua(keys)))\n",
    "        #  score: [N, max_L*(enc_nlayers*directions), 1] -> [N, max_L*(enc_nlayers*directions)] -> [N, 1, max_L*(enc_nlayers*directions)]  \n",
    "        scores=scores.squeeze().unsqueeze(1)\n",
    "        \n",
    "        #softmax(weights)\n",
    "        weights=F.softmax(scores,dim=-1)\n",
    "        #  [N,1,max_L*(enc_nlayers*directions)] @ [N,max_L*(enc_nlayers*directions),enc_hidden_size] -> [N,1,enc_hidden_size]\n",
    "        context=torch.bmm(weights,keys) \n",
    "\n",
    "        return context,weights\n",
    "\n",
    "class BahdanauDecoder(nn.Module):\n",
    "    def __init__(self,vocab_size,hidden_size,num_layers,encoding_dict):\n",
    "        super(BahdanauDecoder,self).__init__()\n",
    "        self.encoding_dict=encoding_dict\n",
    "        self.encoding_dict['dec_hidden_size']=hidden_size                \n",
    "        self.encoding_dict['dec_num_layers']=num_layers\n",
    "        self.embedding=nn.Embedding(vocab_size,hidden_size)                            # Embedding Layer: [ vocab_size, enc_hidden_dims ]\n",
    "        self.attention=BahdanauAttention(self.encoding_dict)                           # Inputs: [t-1_dec_hidden_state,all_enc_hidden_states] | Outputs: [context_vector,attention_weights]\n",
    "        self.gru=nn.GRU(\n",
    "            input_size=self.encoding_dict['enc_hidden_size']+self.encoding_dict['dec_hidden_size'],       #Inputs: [context+input_concat,decoder_hidden_state]\n",
    "            hidden_size=self.encoding_dict['dec_hidden_size'],                                       #Outputs: [lstm_final_layer_activations,dec_hidden_state]\n",
    "            num_layers=self.encoding_dict['dec_num_layers'],\n",
    "            batch_first=True\n",
    "            )\n",
    "        self.fcout=nn.Linear(self.encoding_dict['dec_hidden_size'],vocab_size)\n",
    "\n",
    "    def forward(self,encoder_hiddens,target_tensor=None):\n",
    "        MAX_LENGTH,N=encoder_hiddens.size(0),encoder_hiddens.size(2)\n",
    "        decoder_input=torch.empty((N,1),dtype=torch.long).fill_(SOS_token).to(device)                   #create empty input\n",
    "        decoder_outputs=[]  #output token cache\n",
    "        attention_weights=[]  #atttention weights cache\n",
    "        decoder_hidden=torch.zeros((self.encoding_dict['dec_num_layers'],N,self.encoding_dict['dec_hidden_size'])).to(device)\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output,decoder_hidden,attn_weight=self.forward_step(decoder_input,encoder_hiddens,decoder_hidden)\n",
    "            if target_tensor!=None:\n",
    "                decoder_input=target_tensor[:,i].unsqueeze(1)               #Teacher Forcing with groundtruth label inputs\n",
    "            else:\n",
    "                _,topi=decoder_output.topk(1)                               #Predicted output to input\n",
    "                decoder_input=topi.squeeze(-1).detach()\n",
    "            \n",
    "            \n",
    "            \n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attention_weights.append(attn_weight)\n",
    "        decoder_outputs=F.log_softmax(torch.cat(decoder_outputs,dim=1),dim=-1)\n",
    "        attention_weights=torch.cat(attention_weights,dim=1)\n",
    "        return decoder_outputs,attention_weights\n",
    "    \n",
    "    def forward_step(self,input,encoder_states,decoder_hidden):\n",
    "        embedded=self.embedding(input)          #[batch_size,1] -> [batch_size,1,decoder_hidden]\n",
    "        context,weights=self.attention(query=decoder_hidden,keys=encoder_states) #[[dec_num_layers,N,dec_hidden];[max_L,(enc_nlayers*directions),N,enc_hidden_size]] ->[N,1,enc_hidden_size];[N,1,max_L]    \n",
    "        nn_inp=torch.cat([context,embedded],dim=-1)                         # [N,1,enc_hidden_size+dec_hidden]\n",
    "        decoder_output,decoder_hidden=self.gru(nn_inp,decoder_hidden)       # [[N,1,enc_hidden_size+dec_hidden], [dec_num_layers,N,dec_hidden]] -> [N,1,dec_hidden_state],[dec_num_layers,N,dec_hidden]                                              \n",
    "        decoder_output=self.fcout(decoder_output)                           # [N,1,dec_hidden_state] -> [N,1,output_vocab_size]\n",
    "        return decoder_output,decoder_hidden,weights                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luong Attention Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self,vocab_size,input_size,hidden_size,num_layers):\n",
    "        super(EncoderLSTM,self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab_size,input_size)\n",
    "        self.lstm=nn.LSTM(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,batch_first=True)\n",
    "    def forward(self,x,state_cell=None):\n",
    "        hiddens=[]\n",
    "        N,max_L=x.shape\n",
    "        for i in range(max_L):\n",
    "            embedded=self.embedding(x[:,i].unsqueeze(1))\n",
    "            if not state_cell:\n",
    "                _,state_cell=self.lstm(embedded)\n",
    "            else:\n",
    "                _,state_cell=self.lstm(embedded,state_cell)\n",
    "            hidden=state_cell[0]\n",
    "            hiddens.append(hidden.unsqueeze(2))\n",
    "        hiddens=torch.cat(hiddens,dim=2)                        # we use only the last hidden layers of both encoder and decoder hidden_states\n",
    "        return None,hiddens[-1]                  # encoder_hidden_state (last layer) [N, MAX_L, encoder_hidden_dims]\n",
    "\n",
    "# encoding_dict={\n",
    "#     'enc_hidden_size':enc_hidden_size,\n",
    "#     'enc_num_layers': enc_num_layers,\n",
    "#     'dec_hidden_size':dec_hidden_size,\n",
    "#     \"dec_num_layers\":dec_num_layers\n",
    "#     \n",
    "#     \n",
    "# }\n",
    "\n",
    "class dotAttention(nn.Module):\n",
    "    def __init__(self,_):                   #added _ for consistency with rest of the multiplicative techniques\n",
    "        super(dotAttention,self).__init__()\n",
    "    def forward(self,dec_hidden_state,enc_hidden_states):                               \n",
    "        dot=torch.bmm(enc_hidden_states,dec_hidden_state.transpose(-1,-2)).transpose(-1,-2)\n",
    "        # dot=torch.einsum(\"N1H,NLH->N1L\",[dec_hidden_state,enc_hidden_states])\n",
    "        attn_w=F.softmax(dot,dim=-1)        # NIL @\n",
    "        context=torch.bmm(attn_w,enc_hidden_states)\n",
    "        return context,attn_w                       #context: N,L,eH\n",
    "        \n",
    "class generalDotAttention(nn.Module):\n",
    "    def __init__(self,encoding_dict):\n",
    "        super(generalDotAttention,self).__init__()\n",
    "        self.W_a=nn.Linear(encoding_dict['enc_hidden_size'],encoding_dict['dec_hidden_size'])\n",
    "    def forward(self,dec_hidden_state,enc_hidden_states):\n",
    "        enc_hidden_alligned=self.W_a(enc_hidden_states)\n",
    "        dec_hidden_state=dec_hidden_state.transpose(-1,-2)                    ## N,1,H -> N,H,1\n",
    "        dot=torch.bmm(enc_hidden_alligned,dec_hidden_state).transpose(-1,-2)  ## N,L,H @ N,H,1 -> N,L,1 -> N,1,L\n",
    "        attn_w=F.softmax(dot,dim=-1)        # N,1,L\n",
    "        context=torch.bmm(attn_w,enc_hidden_states)  ## N,1,L @ N,L,eH\n",
    "        return context,attn_w                               #context: N,1,eH\n",
    "        \n",
    "class concatAttention(nn.Module):\n",
    "    def __init__(self,encoding_dict):\n",
    "        super(concatAttention,self).__init__()\n",
    "        self.W_a=nn.Linear(encoding_dict['enc_hidden_size']+encoding_dict['dec_hidden_size'],encoding_dict['dec_hidden_size'])\n",
    "        self.v_a=nn.Linear(encoding_dict['dec_hidden_size'],1)\n",
    "    def forward(self,dec_hidden_state,enc_hidden_states):\n",
    "        print(dec_hidden_state.shape,enc_hidden_states.shape)\n",
    "        concat_=torch.concat((dec_hidden_state.expand(dec_hidden_state.size(0),enc_hidden_states.size(1),dec_hidden_state.size(2)),enc_hidden_states),dim=-1)      #N,1,dH -> N,L,dH c N,L,eH -> N,L,dH+eH  (dH==H)\n",
    "        merged=torch.tanh(self.W_a(concat_))                #N,L,dH+eH -> N,L,H        \n",
    "        attn_w=F.softmax(self.v_a(merged).transpose(-1,-2),dim=-1)  #N,L,H -> softmax(N,1,L)\n",
    "        context=torch.bmm(attn_w,enc_hidden_states)     ## N,1,L @ N,L,eH\n",
    "        return context,attn_w                     #context: N,1,eH ; attn_w:N,1,L\n",
    "\n",
    "\n",
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self,encoding_dict:dict,max_L_d:Optional[tuple]=None,variant:str='general',type_:Optional[str]='global'):\n",
    "        super(LuongAttention,self).__init__()\n",
    "        self.attn_={'concat':concatAttention,'general':generalDotAttention,'dot':dotAttention}\n",
    "        self.attention=self.attn_[variant](encoding_dict)\n",
    "        self.type_=type_\n",
    "        if max_L_d:\n",
    "            max_L,d=max_L_d\n",
    "            if d>max_L:\n",
    "                raise Exception(\"Sorry, but local span cannot be greater than the max sequence length\")\n",
    "            self.seq_len=max_L\n",
    "            self.d=d\n",
    "            self.W_p=nn.Linear(encoding_dict['dec_hidden_size'],1)\n",
    "            self.V_p=nn.Linear(encoding_dict['max_L'],1)\n",
    "\n",
    "\n",
    "    def local_p(self,decoder_hidden_state):\n",
    "        dec_sum=torch.tanh(self.W_p(decoder_hidden_state)).squeeze().unsqueeze(0)\n",
    "        L_ratio=torch.sigmoid(self.V_p(dec_sum))\n",
    "        return self.seq_len*L_ratio.squeeze()\n",
    "\n",
    "    def forward(self,dec_hidden_state,enc_hidden_states):\n",
    "        if self.type_=='local':\n",
    "            p=self.local_p(dec_hidden_state)\n",
    "            enc_hidden_states=enc_hidden_states[:,p-self.d:p+self.d+1,:]                    #[N,max_L,hidden_dims] -> [N,p-d:p+d,hidden_dims]\n",
    "        context,attn_weights=self.attention(dec_hidden_state,enc_hidden_states)\n",
    "        return context,attn_weights\n",
    "    \n",
    "\n",
    "class LuongDecoder(nn.Module):\n",
    "    def __init__(self,vocab_size,hidden_size,num_layers,attn_variant,attn_type,encoding_dict):\n",
    "        super(LuongDecoder,self).__init__()\n",
    "        encoding_dict['dec_hidden_size']=hidden_size\n",
    "        encoding_dict['dec_num_layers']=num_layers\n",
    "        self.embedding=nn.Embedding(vocab_size,hidden_size)\n",
    "        self.lstm=nn.LSTM(input_size=hidden_size,hidden_size=hidden_size,num_layers=num_layers,batch_first=True)\n",
    "        self.attention=LuongAttention(encoding_dict,variant=attn_variant,type_=attn_type)\n",
    "        self.W_c=nn.Linear(encoding_dict[\"enc_hidden_size\"]+encoding_dict[\"dec_hidden_size\"],hidden_size)\n",
    "        self.fcout=nn.Linear(hidden_size,vocab_size)\n",
    "        \n",
    "    def forward(self,enc_hidden_states,target_tensor=None):\n",
    "        N,max_L=enc_hidden_states.shape[:2]\n",
    "        decoder_input=torch.empty((N,1),dtype=torch.long,device=device).fill_(SOS_token)\n",
    "        dec_hidden_cell=None\n",
    "        dec_outputs,attn_weights=[],[]\n",
    "        for i in range(max_L):\n",
    "            decoder_output,dec_hidden_cell,attn_w=self.forward_step(decoder_input,enc_hidden_states,dec_hidden_cell)\n",
    "            if target_tensor==None:\n",
    "                _,topi=decoder_output.topk(1)                               # N,1,vocab_size -> N,1,1                      \n",
    "                decoder_input=topi.squeeze(-1).detach()                     # N,1,1 -> N,1 \n",
    "            else:\n",
    "                decoder_input=target_tensor[:,i].unsqueeze(1)               # N -> N,1\n",
    "            dec_outputs.append(decoder_output)\n",
    "            attn_weights.append(attn_w)\n",
    "        dec_outputs=F.log_softmax(torch.cat(dec_outputs,dim=1),dim=-1)\n",
    "        attn_weights=torch.cat(attn_weights,dim=1)\n",
    "        return dec_outputs,attn_weights            \n",
    "\n",
    "\n",
    "    def forward_step(self,input,enc_hidden_states,dec_hidden_cell=None):\n",
    "        embedded=self.embedding(input)\n",
    "        if not dec_hidden_cell:\n",
    "            _,dec_hidden_cell=self.lstm(embedded)\n",
    "        else:\n",
    "            _,dec_hidden_cell=self.lstm(embedded,dec_hidden_cell)\n",
    "        dec_hidden_state=dec_hidden_cell[0][-1].unsqueeze(1)      #(hidden,cell) -> hidden: Nlayers,N,dH -> N,dH ->  N,1,dH \n",
    "        context,attn_w=self.attention(dec_hidden_state,enc_hidden_states)\n",
    "        context_cat=torch.cat([context,embedded],dim=-1)            # N,1,dH ; N,1,eH -> N,1,dH+eH\n",
    "        decoder_output=torch.tanh(self.W_c(context_cat))            # N,1,dH+eH -> N,1,dH\n",
    "        # decoder_output,decoder_hidden_cell=self.lstm(decoder_output,dec_hidden_cell)\n",
    "        decoder_output=self.fcout(decoder_output)                   # N,1,dH -> N,1,vocab_size\n",
    "        return decoder_output,dec_hidden_cell,attn_w\n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Plotting Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points,range(len(points)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def eval_train(encoder,decoder,data_loader,accuracy):\n",
    "    encoder.eval(),decoder.eval()\n",
    "    accuracy_=0\n",
    "    n_eval=0\n",
    "    with torch.no_grad():\n",
    "        for idx,(input_,targets) in enumerate(data_loader):\n",
    "            _,encoder_hidden=encoder(input_)\n",
    "            preds,attn_weights=decoder(encoder_hidden)\n",
    "            for pred,target in zip(preds,targets):\n",
    "                accuracy_+=accuracy(pred,target)\n",
    "                n_eval+=1\n",
    "    encoder.train(),decoder.train()\n",
    "    return accuracy_.item()/n_eval\n",
    "\n",
    "\n",
    "def train_bahdanau_luong_epoch(encoder,decoder,dataloader,criterion,encoder_optimizer,decoder_optimizer):\n",
    "    total_loss=0\n",
    "    for data in tqdm(dataloader):\n",
    "        x,y=data\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        _,encoder_hidden=encoder(x)\n",
    "        decoder_outputs,attn_weights=decoder(encoder_hidden,target_tensor=y)\n",
    "        loss=criterion(decoder_outputs.reshape(-1,decoder_outputs.size(-1)),y.view(-1))\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss+=loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def train_bahdanau_luong(epochs,encoder,decoder,train_dataloader,test_dataloader,lr,print_every,plot_every):\n",
    "    plot_losses=[]\n",
    "    print_losses=[]\n",
    "    plot_loss_total=0\n",
    "    print_loss_total=0\n",
    "    decoder_optimizer=torch.optim.Adam(params=decoder.parameters(),lr=lr)\n",
    "    encoder_optimizer=torch.optim.Adam(params=encoder.parameters(),lr=lr)\n",
    "    criterion=nn.NLLLoss()\n",
    "    accuracy=Accuracy(task=\"multiclass\",num_classes=decoder.fcout.out_features,top_k=2).to(device)\n",
    "    # pbar=tqdm(total=epochs,position=0,desc='Training Progress')\n",
    "    # loss_log=tqdm(position=1,total=0,bar_foramt='{desc}')\n",
    "    for i in range(1,epochs+1):\n",
    "        avg_loss=train_bahdanau_luong_epoch(encoder,decoder,train_dataloader,criterion,encoder_optimizer,decoder_optimizer)\n",
    "        val_acc=eval_train(encoder,decoder,test_dataloader,accuracy)\n",
    "        plot_loss_total+=avg_loss\n",
    "        print_loss_total+=avg_loss\n",
    "        if i%print_every==0:\n",
    "            print_losses.append(print_loss_total/print_every)\n",
    "            print(f\"Epoch {i} / {epochs} :  Loss {print_loss_total/print_every}   |   Validation Accuracy {val_acc}\")\n",
    "            print_loss_total=0\n",
    "        # loss_log.set_description_str(f\"Epoch {i} / {epochs} :  Loss {print_loss_total/print_every}\")\n",
    "        # pbar.update(1)\n",
    "        if i%plot_every==0:\n",
    "            plot_losses.append(plot_loss_total/plot_every)\n",
    "            plot_loss_total=0\n",
    "    showPlot(plot_losses)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 5173\n",
      "eng 3388\n"
     ]
    }
   ],
   "source": [
    "enc_hidden_size=512\n",
    "enc_num_layers=2\n",
    "encoding_dict={'enc_hidden_size':enc_hidden_size,\"enc_num_layers\":enc_num_layers}\n",
    "input_lang,output_lang,train_data,test_data=get_dataloader('eng','fra',25)\n",
    "enlstm=EncoderLSTM(vocab_size=input_lang.n_words,input_size=256,hidden_size=enc_hidden_size,num_layers=enc_num_layers).to(device)\n",
    "dclstm=LuongDecoder(vocab_size=output_lang.n_words,hidden_size=512,num_layers=1,attn_variant='general',attn_type='global',encoding_dict=encoding_dict).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:38<00:00, 12.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100 :  Loss 0.6604812208667816\n",
      "Epoch 1 / 100 :  Val Accuracy 0.8900459605008135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:38<00:00, 12.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 / 100 :  Loss 0.3170104822719911\n",
      "Epoch 2 / 100 :  Val Accuracy 0.9072993747308097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:38<00:00, 12.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 / 100 :  Loss 0.17152406062826095\n",
      "Epoch 3 / 100 :  Val Accuracy 0.9281772303909839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 26/498 [00:02<00:37, 12.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 2\u001b[0m train_bahdanau_luong(epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,encoder\u001b[39m=\u001b[39;49menbahdanau,decoder\u001b[39m=\u001b[39;49mdecbahdanau,train_dataloader\u001b[39m=\u001b[39;49mtrain_data,test_dataloader\u001b[39m=\u001b[39;49mtest_data,lr\u001b[39m=\u001b[39;49m\u001b[39m0.002\u001b[39;49m,print_every\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,plot_every\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[80], line 48\u001b[0m, in \u001b[0;36mtrain_bahdanau_luong\u001b[0;34m(epochs, encoder, decoder, train_dataloader, test_dataloader, lr, print_every, plot_every)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m# pbar=tqdm(total=epochs,position=0,desc='Training Progress')\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m# loss_log=tqdm(position=1,total=0,bar_foramt='{desc}')\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> 48\u001b[0m     avg_loss\u001b[39m=\u001b[39mtrain_bahdanau_luong_epoch(encoder,decoder,train_dataloader,criterion,encoder_optimizer,decoder_optimizer)\n\u001b[1;32m     49\u001b[0m     val_acc\u001b[39m=\u001b[39mevaluate_bahdanau_luong(encoder,decoder,test_dataloader,accuracy)\n\u001b[1;32m     50\u001b[0m     plot_loss_total\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mavg_loss\n",
      "Cell \u001b[0;32mIn[80], line 27\u001b[0m, in \u001b[0;36mtrain_bahdanau_luong_epoch\u001b[0;34m(encoder, decoder, dataloader, criterion, encoder_optimizer, decoder_optimizer)\u001b[0m\n\u001b[1;32m     24\u001b[0m decoder_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     26\u001b[0m _,encoder_hidden\u001b[39m=\u001b[39mencoder(x)\n\u001b[0;32m---> 27\u001b[0m decoder_outputs,attn_weights\u001b[39m=\u001b[39mdecoder(encoder_hidden,target_tensor\u001b[39m=\u001b[39;49my)\n\u001b[1;32m     28\u001b[0m loss\u001b[39m=\u001b[39mcriterion(decoder_outputs\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,decoder_outputs\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)),y\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     29\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov8/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[14], line 86\u001b[0m, in \u001b[0;36mBahdanauDecoder.forward\u001b[0;34m(self, encoder_hiddens, target_tensor)\u001b[0m\n\u001b[1;32m     84\u001b[0m decoder_hidden\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mzeros((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding_dict[\u001b[39m'\u001b[39m\u001b[39mdec_num_layers\u001b[39m\u001b[39m'\u001b[39m],N,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding_dict[\u001b[39m'\u001b[39m\u001b[39mdec_hidden_size\u001b[39m\u001b[39m'\u001b[39m]))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     85\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(MAX_LENGTH):\n\u001b[0;32m---> 86\u001b[0m     decoder_output,decoder_hidden,attn_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_step(decoder_input,encoder_hiddens,decoder_hidden)\n\u001b[1;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m target_tensor\u001b[39m!=\u001b[39m\u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m         decoder_input\u001b[39m=\u001b[39mtarget_tensor[:,i]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)               \u001b[39m#Teacher Forcing with groundtruth label inputs\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 105\u001b[0m, in \u001b[0;36mBahdanauDecoder.forward_step\u001b[0;34m(self, input, encoder_states, decoder_hidden)\u001b[0m\n\u001b[1;32m    103\u001b[0m context,weights\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(query\u001b[39m=\u001b[39mdecoder_hidden,keys\u001b[39m=\u001b[39mencoder_states) \u001b[39m#[[dec_num_layers,N,dec_hidden];[max_L,(enc_nlayers*directions),N,enc_hidden_size]] ->[N,1,enc_hidden_size];[N,1,max_L]    \u001b[39;00m\n\u001b[1;32m    104\u001b[0m nn_inp\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcat([context,embedded],dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)                         \u001b[39m# [N,1,enc_hidden_size+dec_hidden]\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m decoder_output,decoder_hidden\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgru(nn_inp,decoder_hidden)       \u001b[39m# [[N,1,enc_hidden_size+dec_hidden], [dec_num_layers,N,dec_hidden]] -> [N,1,dec_hidden_state],[dec_num_layers,N,dec_hidden]                                              \u001b[39;00m\n\u001b[1;32m    106\u001b[0m decoder_output\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfcout(decoder_output)                           \u001b[39m# [N,1,dec_hidden_state] -> [N,1,output_vocab_size]\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39mreturn\u001b[39;00m decoder_output,decoder_hidden,weights\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov8/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov8/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1015\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msqueeze(batch_dim)\n\u001b[1;32m   1013\u001b[0m     hidden \u001b[39m=\u001b[39m hidden\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m-> 1015\u001b[0m \u001b[39mreturn\u001b[39;00m output, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpermute_hidden(hidden, unsorted_indices)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "train_bahdanau_luong(epochs=10,encoder=enbahdanau,decoder=decbahdanau,train_dataloader=train_data,test_dataloader=test_data,lr=0.002,print_every=1,plot_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_hidden_size=512\n",
    "enc_num_layers=2\n",
    "device=torch.device('cuda')\n",
    "encoding_dict={'enc_hidden_size':enc_hidden_size,\"enc_num_layers\":enc_num_layers,'directions':2}\n",
    "enbahdanau=EncoderBiRNN(vocab_size=input_lang.n_words,num_layers=1,input_size=256,hidden_size=512).to(device)\n",
    "decbahdanau=BahdanauDecoder(vocab_size=output_lang.n_words,hidden_size=512,num_layers=1,encoding_dict=encoding_dict).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 13056 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 5161\n",
      "eng 3378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['je suis en forme', 'i m fit']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepareData('eng','fra',reverse=True)[2][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target are to be padded\n",
    "T = 50      # Input sequence length\n",
    "C = 20      # Number of classes (including blank)\n",
    "N = 16      # Batch size\n",
    "S = 30      # Target sequence length of longest target in batch (padding length)\n",
    "S_min = 10  # Minimum target length, for demonstration purposes\n",
    "# Initialize random batch of input vectors, for *size = (T,N,C)\n",
    "input = torch.randn(T, N, C).log_softmax(-2).detach().requires_grad_()\n",
    "# Initialize random batch of targets (0 = blank, 1:C = classes)\n",
    "target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)\n",
    "input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)\n",
    "ctc_loss = nn.CTCLoss()\n",
    "loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
    "loss.backward()\n",
    "# Target are to be un-padded\n",
    "T = 50      # Input sequence length\n",
    "C = 20      # Number of classes (including blank)\n",
    "N = 16      # Batch size\n",
    "# Initialize random batch of input vectors, for *size = (T,N,C)\n",
    "input = torch.randn(T, N, C).log_softmax(-1).detach().requires_grad_()\n",
    "input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "# Initialize random batch of targets (0 = blank, 1:C = classes)\n",
    "target_lengths = torch.randint(low=1, high=T, size=(N,), dtype=torch.long)\n",
    "target = torch.randint(low=1, high=C, size=(sum(target_lengths),), dtype=torch.long)\n",
    "ctc_loss = nn.CTCLoss()\n",
    "loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
    "loss.backward()\n",
    "# Target are to be un-padded and unbatched (effectively N=1)\n",
    "T = 50      # Input sequence length\n",
    "C = 20      # Number of classes (including blank)\n",
    "# Initialize random batch of input vectors, for *size = (T,C)\n",
    "input = torch.randn(T, C).log_softmax(-2).detach().requires_grad_()\n",
    "input_lengths = torch.tensor(T, dtype=torch.long)\n",
    "# Initialize random batch of targets (0 = blank, 1:C = classes)\n",
    "target_lengths = torch.randint(low=1, high=T, size=(), dtype=torch.long)\n",
    "target = torch.randint(low=1, high=C, size=(target_lengths,), dtype=torch.long)\n",
    "ctc_loss = nn.CTCLoss()\n",
    "loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6, 37, 34, 48, 16, 40, 21, 37,  2,  1, 29, 31, 40, 47, 27, 38])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(low=1, high=T, size=(N,), dtype=torch.long)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

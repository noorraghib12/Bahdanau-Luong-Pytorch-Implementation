{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import sys\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import Accuracy\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "import time\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocab class for input and output language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    \"\"\"\n",
    "    Lang(self,name:str)\n",
    "    Attributes:\n",
    "        word2index {dict}: Dictionary for mapping word to vocab indexes\n",
    "        index2word {dict}: Dictionary for mapping vocab index to words\n",
    "        word2count {count}: Dictionary for mapping word to its frequency of appearance in dataset\n",
    "    Methods:\n",
    "        addSentence:\n",
    "            args:\n",
    "                sentence {str}\n",
    "            Adds words from sentence to vocab \n",
    "        addWord\n",
    "    \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read languages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Language instances for vocabulary utilizations\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 30\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p,reverse):\n",
    "    if reverse is not True:\n",
    "        return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "            len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "            p[0].startswith(eng_prefixes)\n",
    "    else:\n",
    "        return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "            len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "            p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs,reverse):\n",
    "    return [pair for pair in pairs if filterPair(pair,reverse)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([1,2,3]).tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess all text from txt -> Create pairs from entire data and add words from data to vocab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 5173\n",
      "eng 3388\n",
      "['elle est toujours en mouvement', 'she s always on the go']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False,test_size=None):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs,reverse)\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True,1000)\n",
    "print(random.choice(pairs))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize data, create tensors of the data and create dataset and data loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=30\n",
    "########################################################################################################\n",
    "  \n",
    "def indexesFromSentence(lang,sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang,sentence):\n",
    "    indexes=indexesFromSentence(lang,sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long,device=device).view(1,-1)\n",
    "\n",
    "def tensorFromPair(pair):\n",
    "    input_tensor=tensorFromSentence(input_lang,pair[0])\n",
    "    output_tensor=tensorFromSentence(output_lang,pair[1])\n",
    "    return(input_tensor,output_tensor)\n",
    "\n",
    "\n",
    "####################################################################################################################################################\n",
    "\n",
    "\n",
    "def get_dataloader(lang1,lang2,batch_size,test_ratio=0.1):\n",
    "    input_lang,output_lang,pairs=prepareData(lang1,lang2,True)\n",
    "    N=len(pairs)\n",
    "    input_ids=torch.zeros(size=(N,MAX_LENGTH),dtype=torch.long)\n",
    "    output_ids=torch.zeros(size=(N,MAX_LENGTH),dtype=torch.long)\n",
    "\n",
    "    for idx,(inp,trg) in enumerate(pairs):\n",
    "        try:\n",
    "            input_tensor,output_tensor=tensorFromPair((inp,trg))\n",
    "        except KeyError:\n",
    "            print(f\"error at {idx}th pair\")\n",
    "            print(inp,trg)\n",
    "            continue\n",
    "        input_ids[idx,:input_tensor.shape[1]]=input_tensor\n",
    "        output_ids[idx,:output_tensor.shape[1]]=output_tensor\n",
    "    \n",
    "    test_size=int(N*test_ratio)\n",
    "        \n",
    "    test_idx=np.random.randint(low=0,high=N,size=test_size)\n",
    "    train_idx=np.setdiff1d(np.arange(N),test_idx)\n",
    "    \n",
    "    train_inp,train_outp=input_ids[train_idx],output_ids[train_idx]\n",
    "    test_inp,test_outp=input_ids[test_idx],output_ids[test_idx]\n",
    "    \n",
    "    train_data=TensorDataset(torch.LongTensor(train_inp).to(device),\n",
    "                             torch.LongTensor(train_outp).to(device)\n",
    "                             )\n",
    "    test_data=TensorDataset(torch.LongTensor(test_inp).to(device),\n",
    "                             torch.LongTensor(test_outp).to(device)\n",
    "                             )\n",
    "    \n",
    "    train_sampler=RandomSampler(train_data)\n",
    "    train_dataloader=DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
    "\n",
    "    test_sampler=RandomSampler(test_data)\n",
    "    test_dataloader=DataLoader(test_data,sampler=test_sampler,batch_size=batch_size)\n",
    "    \n",
    "    return input_lang,output_lang,train_dataloader,test_dataloader\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL BUILDING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self,input_dim,heads):\n",
    "        super().__init__()\n",
    "        head_dim=input_dim//heads\n",
    "        self.heads=heads\n",
    "        self.head_dim=head_dim\n",
    "        assert input_dim%heads==0\n",
    "\n",
    "        self.query=nn.Linear(head_dim,head_dim)\n",
    "        self.key=nn.Linear(head_dim,head_dim)\n",
    "        self.value=nn.Linear(head_dim,head_dim)\n",
    "        self.fc_out=nn.Linear(input_dim,input_dim)\n",
    "    \n",
    "    def forward(self,queries,keys,values):\n",
    "        N=queries.shape[0]\n",
    "        ql,kl,vl=queries.shape[1],keys.shape[1],values.shape[1]   #Q,K,V\n",
    "        queries=torch.transpose(queries.reshape(N,ql,self.heads,self.head_dim),1,2) #NHQD\n",
    "        keys=torch.transpose(keys.reshape(N,kl,self.heads,self.head_dim),1,2)   #NHKD\n",
    "        values=torch.transpose(keys.reshape(N,vl,self.heads,self.head_dim),1,2)   #NHVD   \n",
    "        queries_=self.query(queries)\n",
    "        keys_=self.key(keys)\n",
    "        values_=self.value(values)\n",
    "\n",
    "        attention=torch.bmm(queries_,torch.transpose(keys_-1,-2))           #NHQD . NHDK -> NHQK\n",
    "        \n",
    "        attention_norm=torch.softmax(attention/(self.head_dim**0.5),axis=3)\n",
    "        attention_out=torch.transpose(torch.bmm(attention_norm,values_),1,2)    #NHQK . NHVD -> NHQD (V=K)\n",
    "        out=self.fc_out(attention_out.reshape(N,ql,self.heads*self.head_dim))\n",
    "        return out \n",
    "        \n",
    "\n",
    "# class TransformerBlock(nn.Module):\n",
    "#     def __init__(self,input_dim,heads,expansion,dropout):\n",
    "#         self.expansion=input_dim*expansion\n",
    "#         self.attention=SelfAttention(input_dim,heads)\n",
    "#         self.dropout=nn.Dropout(dropout)\n",
    "#         self.norm1=nn.LayerNorm(input_dim)\n",
    "#         self.norm2=nn.LayerNorm(input_dim)\n",
    "#         self.expansion_layer=nn.Sequential(\n",
    "#             nn.Linear(input_dim,self.expansion),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(expansion,input_dim)\n",
    "#         )   \n",
    "\n",
    "#     def forward(self,query,key,value,mask):\n",
    "#         attention=self.attention(query,key,value)\n",
    "\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,dropout_p=0.1,vocab_size=None):\n",
    "        super(EncoderRNN,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding=nn.Embedding(vocab_size,hidden_size)\n",
    "        self.gru=nn.GRU(hidden_size,hidden_size,batch_first=True)\n",
    "        self.dropout=nn.Dropout(dropout_p)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        embedded=self.dropout(self.embedding(input))\n",
    "        out,hidden_state=self.gru(embedded)\n",
    "        return out,hidden_state\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,output_size,hidden_size,vocab_size=None):\n",
    "        super(DecoderRNN,self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab_size,hidden_size)\n",
    "        self.gru=nn.GRU(hidden_size,hidden_size,batch_first=True)\n",
    "        self.out=nn.Linear(hidden_size,output_size)\n",
    "    \n",
    "    def forward_step(self,input,hidden):\n",
    "        embedded=self.embedding(input)\n",
    "        output,hidden=self.gru(embedded,hidden)\n",
    "        output=self.out(output)\n",
    "        return output,hidden \n",
    "\n",
    "    def forward(self,encoder_outputs,encoder_hidden,target_tensor=None):\n",
    "        N=encoder_outputs.size(0)\n",
    "        decoder_input=torch.empty(N,1,dtype=torch.long,device=device).fill_(SOS_token)\n",
    "        decoder_hidden=encoder_hidden\n",
    "        decoder_outputs=[]\n",
    "        for i in range(MAX_LENGTH):\n",
    "            gru_out,decoder_hidden=self.forward_step(decoder_input,decoder_hidden)\n",
    "            decoder_outputs.append(gru_out)\n",
    "            if target_tensor is not None:\n",
    "                decoder_input=target_tensor[:,i].unsqueeze(1)\n",
    "\n",
    "            else:\n",
    "                _,topi=gru_out.topk(1)\n",
    "                decoder_input=topi.squeeze(-1).detach()\n",
    "\n",
    "        \n",
    "        decoder_outputs=torch.cat(decoder_outputs,dim=1)\n",
    "        decoder_outputs=F.log_softmax(decoder_outputs,dim=-1)\n",
    "        return decoder_outputs,decoder_hidden,None\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bahdanau Attention Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBiRNN(nn.Module):\n",
    "    def __init__(self,vocab_size,num_layers,input_size,hidden_size):\n",
    "        super(EncoderBiRNN,self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab_size,input_size)\n",
    "        self.BiRNN=nn.GRU(input_size,hidden_size, num_layers=num_layers,bidirectional=True,batch_first=True)\n",
    "        \n",
    "    def forward(self,x,hidden=None):\n",
    "        N,max_L=x.shape\n",
    "        hiddens=[]\n",
    "        outputs=[]\n",
    "        for i in range(max_L):\n",
    "            embedding=self.embedding(x[:,i].unsqueeze(1))\n",
    "            output,hidden=self.BiRNN(embedding,hidden)\n",
    "            outputs.append(output)\n",
    "            hiddens.append(hidden.unsqueeze(0))\n",
    "        outputs=torch.cat(outputs,dim=1)            # outputs [max_L,N,1,enc_hidden_size]\n",
    "        hiddens=torch.cat(hiddens,dim=0)            # hiddens [max_L,enc_nlayers*directions,N,enc_hidden_size]\n",
    "\n",
    "        return outputs,hiddens\n",
    "    \n",
    "    #encoding_dict={directions=enc_directions,\n",
    "        # enc_hidden_size=enc_hidden_size,\n",
    "        # dec_hidden_size=enc_hidden_size,\n",
    "        # enc_num_layers=enc_num_layers,\n",
    "        # dec_num_layers=dec_num_layers}\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self,encoding_dict):\n",
    "        super(BahdanauAttention,self).__init__()\n",
    "        self.encoding_dict=encoding_dict\n",
    "        self.Wa=nn.Linear(self.encoding_dict['dec_hidden_size']*self.encoding_dict['dec_num_layers'],self.encoding_dict['dec_hidden_size'])\n",
    "        self.Ua=nn.Linear(self.encoding_dict[\"enc_hidden_size\"],self.encoding_dict['dec_hidden_size'])\n",
    "        self.Va=nn.Linear(self.encoding_dict[\"dec_hidden_size\"],1)\n",
    "\n",
    "    def forward(self,query,keys): \n",
    "        #query [dec_num_layers,N,dec_hidden]; keys  [max_L,(enc_nlayers*directions),N,enc_hidden_size]  \n",
    "        #  [dec_num_layers,N,dec_hidden]-> [N,dec_num_layers,dec_hidden]-> [N,1,dec_num_layers*dec_hidden]\n",
    "        query=query.transpose(0,1)\n",
    "        query=query.reshape(query.size(0),1,query.size(1)*query.size(2))    \n",
    "        #  [max_L,(enc_nlayers*directions),N,enc_hidden_size] -> [max_L*(enc_nlayers*directions),N,enc_hidden_size] -> [N,max_L*(enc_nlayers*directions),enc_hidden_size]\n",
    "        keys=keys.reshape(-1,keys.size(2),keys.size(3)).transpose(0,1)\n",
    "        #  query: [N, 1, dec_num_layers*dec_hidden] @ [dec_hidden*dec_num_layers, dec_hidden] -> [N, 1, dec_hidden]\n",
    "        #  keys:  [N, max_L*(enc_nlayers*directions), enc_hidden_size] @ [enc_hidden_size, dec_hidden] -> [N, max_L*(enc_nlayers*directions), dec_hidden]\n",
    "        \n",
    "        #  score(addition): [N, max_L*(enc_nlayers*directions), dec_hidden] + [N, 1, dec_hidden](broadcasted) -> [N, max_L*(enc_nlayers*directions), dec_hidden]\n",
    "        #  score(reduce_sum ): [N, max_L*(enc_nlayers*directions), dec_hidden] @ [dec_hidden, 1] -> [N, max_L*(enc_nlayers*directions), 1]                \n",
    "        scores=self.Va(torch.tanh(self.Wa(query)+self.Ua(keys)))\n",
    "        #  score: [N, max_L*(enc_nlayers*directions), 1] -> [N, max_L*(enc_nlayers*directions)] -> [N, 1, max_L*(enc_nlayers*directions)]  \n",
    "        scores=scores.squeeze().unsqueeze(1)\n",
    "        \n",
    "        #softmax(weights)\n",
    "        weights=F.softmax(scores,dim=-1)\n",
    "        #  [N,1,max_L*(enc_nlayers*directions)] @ [N,max_L*(enc_nlayers*directions),enc_hidden_size] -> [N,1,enc_hidden_size]\n",
    "        context=torch.bmm(weights,keys) \n",
    "\n",
    "        return context,weights\n",
    "\n",
    "class BahdanauDecoder(nn.Module):\n",
    "    def __init__(self,vocab_size,hidden_size,num_layers,encoding_dict):\n",
    "        super(BahdanauDecoder,self).__init__()\n",
    "        self.encoding_dict=encoding_dict\n",
    "        self.encoding_dict['dec_hidden_size']=hidden_size                \n",
    "        self.encoding_dict['dec_num_layers']=num_layers\n",
    "        self.embedding=nn.Embedding(vocab_size,hidden_size)                            # Embedding Layer: [ vocab_size, enc_hidden_dims ]\n",
    "        self.gru=nn.GRU(\n",
    "            input_size=self.encoding_dict['enc_hidden_size']+self.encoding_dict['dec_hidden_size'],       #Inputs: [context+input_concat,decoder_hidden_state]\n",
    "            hidden_size=self.encoding_dict['dec_hidden_size'],                                       #Outputs: [lstm_final_layer_activations,dec_hidden_state]\n",
    "            num_layers=self.encoding_dict['dec_num_layers'],\n",
    "            batch_first=True,\n",
    "            bidirectional=encoding_dict['bidirectional']\n",
    "            )\n",
    "        if self.gru.bidirectional:\n",
    "            self.encoding_dict['dec_directions']=2\n",
    "            self.encoding_dict['dec_num_layers']*=self.encoding_dict['dec_directions']\n",
    "        else:\n",
    "            self.encoding_dict['dec_directions']=1                                    # In the case our decoder is bidirectional we would need to change the \n",
    "            \n",
    "        self.attention=BahdanauAttention(self.encoding_dict)                           # Inputs: [t-1_dec_hidden_state,all_enc_hidden_states] | Outputs: [context_vector,attention_weights]\n",
    "        self.fcout=nn.Linear(self.encoding_dict['dec_hidden_size']*self.encoding_dict['dec_directions'],vocab_size)\n",
    "\n",
    "    def forward(self,encoder_hiddens,target_tensor=None):\n",
    "        MAX_LENGTH,N=encoder_hiddens.size(0),encoder_hiddens.size(2)\n",
    "        decoder_input=torch.empty((N,1),dtype=torch.long).fill_(SOS_token).to(device)                   #create empty input\n",
    "        decoder_outputs=[]  #output token cache\n",
    "        attention_weights=[]  #atttention weights cache\n",
    "        decoder_hidden=torch.zeros((self.encoding_dict['dec_num_layers'],N,self.encoding_dict['dec_hidden_size'])).to(device)\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output,decoder_hidden,attn_weight=self.forward_step(decoder_input,encoder_hiddens,decoder_hidden)\n",
    "            if target_tensor!=None:\n",
    "                decoder_input=target_tensor[:,i].unsqueeze(1)               #Teacher Forcing with groundtruth label inputs\n",
    "            else:\n",
    "                _,topi=decoder_output.topk(1)                               #Predicted output to input\n",
    "                decoder_input=topi.squeeze(-1).detach()\n",
    "            \n",
    "            \n",
    "            \n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attention_weights.append(attn_weight)\n",
    "        decoder_outputs=F.log_softmax(torch.cat(decoder_outputs,dim=1),dim=-1)\n",
    "        attention_weights=torch.cat(attention_weights,dim=1)\n",
    "        return decoder_outputs,attention_weights\n",
    "    \n",
    "    def forward_step(self,input,encoder_states,decoder_hidden):\n",
    "        embedded=self.embedding(input)          #[batch_size,1] -> [batch_size,1,decoder_hidden]\n",
    "        context,weights=self.attention(query=decoder_hidden,keys=encoder_states) #[[dec_num_layers,N,dec_hidden];[max_L,(enc_nlayers*directions),N,enc_hidden_size]] ->[N,1,enc_hidden_size];[N,1,max_L]    \n",
    "        nn_inp=torch.cat([context,embedded],dim=-1)                         # [N,1,enc_hidden_size+dec_hidden]\n",
    "        decoder_output,decoder_hidden=self.gru(nn_inp,decoder_hidden)       # [[N,1,enc_hidden_size+dec_hidden], [dec_num_layers,N,dec_hidden]] -> [N,1,dec_hidden_state],[dec_num_layers,N,dec_hidden]                                              \n",
    "        decoder_output=self.fcout(decoder_output)                           # [N,1,dec_hidden_state] -> [N,1,output_vocab_size]\n",
    "        return decoder_output,decoder_hidden,weights                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luong Attention Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self,vocab_size,input_size,hidden_size,num_layers,encoding_dict):\n",
    "        super(EncoderLSTM,self).__init__()\n",
    "        self.encoding_dict=encoding_dict\n",
    "        self.embedding=nn.Embedding(vocab_size,input_size)\n",
    "        self.lstm=nn.LSTM(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,batch_first=True,bidirectional=self.encoding_dict['enc_bidirection'])\n",
    "         \n",
    "    def forward(self,x,state_cell=None):\n",
    "        hiddens=[]\n",
    "        N,max_L=x.shape\n",
    "        for i in range(max_L):\n",
    "            embedded=self.embedding(x[:,i].unsqueeze(1))\n",
    "            if not state_cell:\n",
    "                _,state_cell=self.lstm(embedded)\n",
    "            else:\n",
    "                _,state_cell=self.lstm(embedded,state_cell)\n",
    "            hidden=state_cell[0]\n",
    "            hiddens.append(hidden.unsqueeze(2))\n",
    "        hiddens=torch.cat(hiddens,dim=2)\n",
    "\n",
    "        #encoder_hidden_state (last layer) [directions,N, MAX_L, encoder_hidden_dims](we use only the last hidden layers of both encoder and decoder hidden_states)\n",
    "        #[directions,N, MAX_L, encoder_hidden_dims] -> [N, MAX_L,directions,encoder_hidden_dims] ->[N, MAX_L,directions*encoder_hidden_dims]\n",
    "        final_layer_hiddens=hiddens[-1*self.encoding_dict['enc_directions']:].permute(1,2,0,3)\n",
    "        final_layer_hiddens=final_layer_hiddens.reshape(final_layer_hiddens.size(0),final_layer_hiddens.size(1),final_layer_hiddens.size(2)*final_layer_hiddens.size(3))\n",
    "        return None, final_layer_hiddens                  #[N, MAX_L,directions*encoder_hidden_dims]\n",
    "\n",
    "# encoding_dict={\n",
    "#     'enc_hidden_size':enc_hidden_size,\n",
    "#     'enc_num_layers': enc_num_layers,\n",
    "#     'dec_hidden_size':dec_hidden_size,\n",
    "#     \"dec_num_layers\":dec_num_layers     \n",
    "# }\n",
    "\n",
    "########################################################### \n",
    "#eH: eH*directions; \n",
    "#H: H*directions; \n",
    "#dH: dH*directions;\n",
    "#hidden_dims: hidden_size*directions; \n",
    "###########################################################\n",
    "\n",
    "class dotAttention(nn.Module):\n",
    "    def __init__(self,encoding_dict):                   #added _ for consistency with rest of the multiplicative techniques\n",
    "        super(dotAttention,self).__init__()\n",
    "        assert encoding_dict['enc_hidden_size']*encoding_dict['enc_directions']==encoding_dict['dec_hidden_size']*encoding_dict['dec_directions'], \"Hidden dims need to be same for both encoder states and decoder states for dotAttention\"\n",
    "    def forward(self,dec_hidden_state,enc_hidden_states):                               \n",
    "        dot=torch.bmm(enc_hidden_states,dec_hidden_state.transpose(-1,-2)).transpose(-1,-2)\n",
    "        # dot=torch.einsum(\"N1H,NLH->N1L\",[dec_hidden_state,enc_hidden_states])\n",
    "        attn_w=F.softmax(dot,dim=-1)        # NIL @\n",
    "        context=torch.bmm(attn_w,enc_hidden_states)\n",
    "        return context,attn_w                       #context: N,L,eH\n",
    "        \n",
    "class generalDotAttention(nn.Module):\n",
    "    def __init__(self,encoding_dict):\n",
    "        super(generalDotAttention,self).__init__()\n",
    "        self.W_a=nn.Linear(encoding_dict['enc_hidden_size']*encoding_dict['enc_directions'],encoding_dict['dec_hidden_size']*encoding_dict['dec_directions'])\n",
    "    def forward(self,dec_hidden_state,enc_hidden_states):\n",
    "        enc_hidden_alligned=self.W_a(enc_hidden_states)\n",
    "        dec_hidden_state=dec_hidden_state.transpose(-1,-2)                    ## N,1,H -> N,H,1\n",
    "        dot=torch.bmm(enc_hidden_alligned,dec_hidden_state).transpose(-1,-2)  ## N,L,H @ N,H,1 -> N,L,1 -> N,1,L\n",
    "        attn_w=F.softmax(dot,dim=-1)        # N,1,L\n",
    "        context=torch.bmm(attn_w,enc_hidden_states)  ## N,1,L @ N,L,eH\n",
    "        return context,attn_w                               #context: N,1,eH       \n",
    "        \n",
    "class concatAttention(nn.Module):\n",
    "    def __init__(self,encoding_dict):\n",
    "        super(concatAttention,self).__init__()\n",
    "        self.W_a=nn.Linear(encoding_dict['enc_hidden_size']*encoding_dict['enc_directions']+encoding_dict['dec_hidden_size']*encoding_dict['dec_directions'],encoding_dict['dec_hidden_size']*encoding_dict['dec_directions'])\n",
    "        self.v_a=nn.Linear(encoding_dict['dec_hidden_size']*encoding_dict['dec_directions'],1)\n",
    "    def forward(self,dec_hidden_state,enc_hidden_states):\n",
    "        concat_=torch.concat((dec_hidden_state.expand(dec_hidden_state.size(0),enc_hidden_states.size(1),dec_hidden_state.size(2)),enc_hidden_states),dim=-1)      #N,1,dH -> N,L,dH c N,L,eH -> N,L,dH+eH  (dH==H)\n",
    "        merged=torch.tanh(self.W_a(concat_))                #N,L,dH+eH -> N,L,H        \n",
    "        attn_w=F.softmax(self.v_a(merged).transpose(-1,-2),dim=-1)  #N,L,H -> softmax(N,1,L)\n",
    "        context=torch.bmm(attn_w,enc_hidden_states)     ## N,1,L @ N,L,eH\n",
    "        return context,attn_w                     #context: N,1,eH ; attn_w:N,1,L\n",
    "\n",
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self,encoding_dict:dict,luong_variant:str='general',luong_type:Optional[str]='global'):\n",
    "        super(LuongAttention,self).__init__()\n",
    "        self.attn_={'concat':concatAttention,'general':generalDotAttention,'dot':dotAttention}\n",
    "        self.attention=self.attn_[luong_variant](encoding_dict)\n",
    "        self.type_=luong_type\n",
    "        if self.type_=='local':\n",
    "            max_L,d=encoding_dict['max_L'],encoding_dict['luong_d']\n",
    "            if d>max_L:\n",
    "                raise IndexError(\"Sorry, but local span cannot be greater than the max sequence length\")\n",
    "            self.max_L=max_L\n",
    "            self.d=d\n",
    "            self.W_p=nn.Linear(encoding_dict['dec_hidden_size']*encoding_dict['dec_directions'],encoding_dict['max_L'])\n",
    "            self.V_p=nn.Linear(encoding_dict['max_L'],1)\n",
    "\n",
    "    def local_p(self,decoder_hidden_state):\n",
    "        dec_sum=torch.tanh(self.W_p(decoder_hidden_state))\n",
    "        L_ratio=torch.sigmoid(self.V_p(dec_sum))\n",
    "        return self.max_L*L_ratio.squeeze().type(torch.int64)\n",
    "\n",
    "    def enc_hidden_localize(self,enc_hidden_states,p):\n",
    "        indexed=[]\n",
    "        max_l=enc_hidden_states.size(1)\n",
    "        assert (self.d>0 and self.d<max_l), \"please make sure window size is greater than zero and less than max sequence length\"\n",
    "        for i,idx in zip(enc_hidden_states,p):\n",
    "            low=idx-self.d if idx-self.d>=0 else 0\n",
    "            high=idx+self.d if idx+self.d<=max_l else max_l\n",
    "            indexed.append(i[low:high,:].unsqueeze(0))\n",
    "        return torch.cat(indexed,dim=0)\n",
    "\n",
    "    def forward(self,dec_hidden_state,enc_hidden_states):\n",
    "        if self.type_=='local':\n",
    "            p=self.local_p(dec_hidden_state)\n",
    "            enc_hidden_states=self.enc_hidden_localize(enc_hidden_states,p)           #[N,max_L,hidden_dims] -> [N,p-d:p+d,hidden_dims]\n",
    "        context,attn_weights=self.attention(dec_hidden_state,enc_hidden_states)\n",
    "        return context,attn_weights\n",
    "\n",
    "class LuongDecoder(nn.Module):\n",
    "    def __init__(self,vocab_size,hidden_size,num_layers,attn_variant,attn_type,encoding_dict):\n",
    "        super(LuongDecoder,self).__init__()\n",
    "        self.encoding_dict=encoding_dict\n",
    "        self.embedding=nn.Embedding(vocab_size,hidden_size)\n",
    "        self.lstm=nn.LSTM(input_size=hidden_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=encoding_dict['dec_bidirection']\n",
    "                          )\n",
    "        \n",
    "        self.attention=LuongAttention(encoding_dict,luong_variant=attn_variant,luong_type=attn_type)\n",
    "        self.W_c=nn.Linear(encoding_dict[\"enc_hidden_size\"]*encoding_dict[\"enc_directions\"]+encoding_dict[\"dec_hidden_size\"],hidden_size)\n",
    "        self.fcout=nn.Linear(hidden_size,vocab_size)\n",
    "        \n",
    "    def forward(self,enc_hidden_states,target_tensor=None):\n",
    "        N,max_L=enc_hidden_states.shape[:2]\n",
    "        decoder_input=torch.empty((N,1),dtype=torch.long,device=device).fill_(SOS_token)\n",
    "        dec_hidden_cell=None\n",
    "        dec_outputs,attn_weights=[],[]\n",
    "        for i in range(max_L):\n",
    "            decoder_output,dec_hidden_cell,attn_w=self.forward_step(decoder_input,enc_hidden_states,dec_hidden_cell)\n",
    "            if target_tensor==None:\n",
    "                _,topi=decoder_output.topk(1)                               # N,1,vocab_size -> N,1,1                      \n",
    "                decoder_input=topi.squeeze(-1).detach()                     # N,1,1 -> N,1 \n",
    "            else:\n",
    "                decoder_input=target_tensor[:,i].unsqueeze(1)               # N -> N,1\n",
    "            dec_outputs.append(decoder_output)\n",
    "            attn_weights.append(attn_w)\n",
    "        dec_outputs=F.log_softmax(torch.cat(dec_outputs,dim=1),dim=-1)\n",
    "        attn_weights=torch.cat(attn_weights,dim=1)\n",
    "        return dec_outputs,attn_weights            \n",
    "\n",
    "\n",
    "    def forward_step(self,input,enc_hidden_states,dec_hidden_cell=None):\n",
    "        embedded=self.embedding(input)\n",
    "        if not dec_hidden_cell:\n",
    "            _,dec_hidden_cell=self.lstm(embedded)\n",
    "        else:\n",
    "            _,dec_hidden_cell=self.lstm(embedded,dec_hidden_cell)\n",
    "        dec_hidden_state=dec_hidden_cell[0][-1*self.encoding_dict['dec_directions']:]\n",
    "        dec_hidden_state=dec_hidden_state.transpose(0,1)\n",
    "        dec_hidden_state=dec_hidden_state.reshape(dec_hidden_state.size(0),dec_hidden_state.size(1)*dec_hidden_state.size(2)).unsqueeze(1)      #(hidden,cell) -> hidden: directions*Nlayers,N,dH -> directions,N,dH -> N,dH*directions ->  N,1,dH*directions \n",
    "        context,attn_w=self.attention(dec_hidden_state,enc_hidden_states)\n",
    "        context.shape\n",
    "        context_cat=torch.cat([context,embedded],dim=-1)            # N,1,dH ; N,1,eH -> N,1,dH+eH\n",
    "        decoder_output=torch.tanh(self.W_c(context_cat))            # N,1,dH+eH -> N,1,dH\n",
    "        # decoder_output,decoder_hidden_cell=self.lstm(decoder_output,dec_hidden_cell)\n",
    "        decoder_output=self.fcout(decoder_output)                   # N,1,dH -> N,1,vocab_size\n",
    "        return decoder_output,dec_hidden_cell,attn_w\n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoding_dict['max_L']=MAX_LENGTH\n",
    "W_p=nn.Linear(encoding_dict['dec_hidden_size']*encoding_dict['dec_directions'],encoding_dict['max_L'])\n",
    "V_p=nn.Linear(encoding_dict['max_L'],1)\n",
    "\n",
    "\n",
    "def local_p(decoder_hidden_state):\n",
    "    dec_sum=torch.tanh(W_p(decoder_hidden_state))\n",
    "    print(dec_sum.shape,V_p.weight.shape)\n",
    "    L_ratio=torch.sigmoid(V_p(dec_sum))\n",
    "    return MAX_LENGTH*L_ratio.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 1, 30]) torch.Size([1, 30])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 6, 256])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=local_p(torch.rand(35,1,encoding_dict['dec_directions']*encoding_dict['dec_hidden_size'])).type(torch.int64)\n",
    "tensor=torch.rand(35,50,256)\n",
    "enc_hidden_encased(tensor,p,3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_hidden_encased(enc_hidden_states,p,d):\n",
    "    indexed=[]\n",
    "    max_l=enc_hidden_states.size(1)\n",
    "    for i,idx in zip(enc_hidden_states,p):\n",
    "        low=idx-d if idx-d>=0 else 0\n",
    "        high=idx+d if idx+d<=max_l else max_l\n",
    "        indexed.append(i[low:high,:].unsqueeze(0))\n",
    "        print(i[idx-d:idx+d,:].shape)\n",
    "    return torch.cat(indexed,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[300], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mrand(\u001b[39m5\u001b[39;49m,\u001b[39m30\u001b[39;49m,\u001b[39m256\u001b[39;49m)\u001b[39m.\u001b[39;49mgather(dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,index\u001b[39m=\u001b[39;49mp)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "torch.rand(5,30,256).gather(dim=1,index=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Plotting Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points,range(len(points)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def eval_train(encoder,decoder,data_loader,accuracy):\n",
    "    encoder.eval(),decoder.eval()\n",
    "    accuracy_=0\n",
    "    n_eval=0\n",
    "    with torch.no_grad():\n",
    "        for idx,(input_,targets) in enumerate(data_loader):\n",
    "            _,encoder_hidden=encoder(input_)\n",
    "            preds,attn_weights=decoder(encoder_hidden)\n",
    "            for pred,target in zip(preds,targets):\n",
    "                accuracy_+=accuracy(pred,target)\n",
    "                n_eval+=1\n",
    "    encoder.train(),decoder.train()\n",
    "    return accuracy_.item()/n_eval\n",
    "\n",
    "\n",
    "def train_bahdanau_luong_epoch(encoder,decoder,dataloader,criterion,encoder_optimizer,decoder_optimizer):\n",
    "    total_loss=0\n",
    "    for data in tqdm(dataloader):\n",
    "        x,y=data\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        _,encoder_hidden=encoder(x)\n",
    "        decoder_outputs,attn_weights=decoder(encoder_hidden,target_tensor=y)\n",
    "        loss=criterion(decoder_outputs.reshape(-1,decoder_outputs.size(-1)),y.view(-1))\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss+=loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def train_bahdanau_luong(epochs,encoder,decoder,train_dataloader,test_dataloader,lr,print_every,plot_every):\n",
    "    plot_losses=[]\n",
    "    print_losses=[]\n",
    "    plot_loss_total=0\n",
    "    print_loss_total=0\n",
    "    decoder_optimizer=torch.optim.Adam(params=decoder.parameters(),lr=lr)\n",
    "    encoder_optimizer=torch.optim.Adam(params=encoder.parameters(),lr=lr)\n",
    "    criterion=nn.NLLLoss()\n",
    "    accuracy=Accuracy(task=\"multiclass\",num_classes=decoder.fcout.out_features,top_k=2).to(device)\n",
    "    # pbar=tqdm(total=epochs,position=0,desc='Training Progress')\n",
    "    # loss_log=tqdm(position=1,total=0,bar_foramt='{desc}')\n",
    "    for i in range(1,epochs+1):\n",
    "        avg_loss=train_bahdanau_luong_epoch(encoder,decoder,train_dataloader,criterion,encoder_optimizer,decoder_optimizer)\n",
    "        val_acc=eval_train(encoder,decoder,test_dataloader,accuracy)\n",
    "        plot_loss_total+=avg_loss\n",
    "        print_loss_total+=avg_loss\n",
    "        if i%print_every==0:\n",
    "            print_losses.append(print_loss_total/print_every)\n",
    "            print(f\"Epoch {i} / {epochs} :  Loss {print_loss_total/print_every}   |   Validation Accuracy {val_acc}\")\n",
    "            print_loss_total=0\n",
    "        # loss_log.set_description_str(f\"Epoch {i} / {epochs} :  Loss {print_loss_total/print_every}\")\n",
    "        # pbar.update(1)\n",
    "        if i%plot_every==0:\n",
    "            plot_losses.append(plot_loss_total/plot_every)\n",
    "            plot_loss_total=0\n",
    "    showPlot(plot_losses)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_hidden_size=512\n",
    "enc_num_layers=3\n",
    "encoding_dict={'enc_hidden_size':enc_hidden_size,\n",
    "               \"enc_num_layers\":enc_num_layers,\n",
    "               'enc_bidirection':True,\n",
    "               'dec_bidirection':False,\n",
    "               'dec_hidden_size':512,\n",
    "               'max_L':30,\n",
    "               'luong_d':3}\n",
    "encoding_dict['enc_directions']=1 if not encoding_dict['enc_bidirection'] else 2\n",
    "encoding_dict['dec_directions']=1 if not encoding_dict['dec_bidirection'] else 2\n",
    "# input_lang,output_lang,train_data,test_data=get_dataloader('eng','fra',25)\n",
    "enlstm=EncoderLSTM(vocab_size=input_lang.n_words,input_size=256,hidden_size=enc_hidden_size,num_layers=enc_num_layers,encoding_dict=encoding_dict).to(device)\n",
    "dclstm=LuongDecoder(vocab_size=output_lang.n_words,hidden_size=512,num_layers=1,attn_variant='concat',attn_type='global',encoding_dict=encoding_dict).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enc_hidden_size': 512,\n",
       " 'enc_num_layers': 3,\n",
       " 'enc_bidirection': True,\n",
       " 'dec_bidirection': False,\n",
       " 'dec_hidden_size': 512,\n",
       " 'max_L': 30,\n",
       " 'luong_d': 3,\n",
       " 'enc_directions': 2,\n",
       " 'dec_directions': 1}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 30, 3388])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_hidden_states=enlstm(x)[1]\n",
    "dclstm(encoder_hidden_states,y)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:09<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10 :  Loss 0.7774928457374815   |   Validation Accuracy 0.8473097458994836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:09<00:00,  6.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 / 10 :  Loss 0.471208248397469   |   Validation Accuracy 0.8939839416363811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:10<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 / 10 :  Loss 0.33037582159293855   |   Validation Accuracy 0.9026050757340283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:08<00:00,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 / 10 :  Loss 0.2356879269146215   |   Validation Accuracy 0.9088778273240244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:08<00:00,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 / 10 :  Loss 0.17024465579586692   |   Validation Accuracy 0.9194110849930662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:05<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 / 10 :  Loss 0.1252174643870396   |   Validation Accuracy 0.9222922992779266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:02<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 / 10 :  Loss 0.09642935445225692   |   Validation Accuracy 0.9275466463633321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:03<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 / 10 :  Loss 0.07773597476776013   |   Validation Accuracy 0.9329536046886955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:03<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 / 10 :  Loss 0.06484233831224437   |   Validation Accuracy 0.9253780537191565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:03<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 10 :  Loss 0.06254603692137868   |   Validation Accuracy 0.9321883069529456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_229231/3006362193.py:8: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "train_bahdanau_luong(epochs=10,encoder=enlstm,decoder=dclstm,train_dataloader=train_data,test_dataloader=test_data,lr=0.002,print_every=1,plot_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_hidden_size=512\n",
    "enc_num_layers=2\n",
    "device=torch.device('cuda')\n",
    "encoding_dict={'enc_hidden_size':enc_hidden_size,\"enc_num_layers\":enc_num_layers,'bidirectional':True}\n",
    "enbahdanau=EncoderBiRNN(vocab_size=input_lang.n_words,num_layers=2,input_size=256,hidden_size=512).to(device)\n",
    "decbahdanau=BahdanauDecoder(vocab_size=output_lang.n_words,hidden_size=512,num_layers=2,encoding_dict=encoding_dict).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:13<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10 :  Loss 0.7413054793318615   |   Validation Accuracy 0.8568235716932862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:10<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 / 10 :  Loss 0.43619052078653486   |   Validation Accuracy 0.8835526738535291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:10<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 / 10 :  Loss 0.3015316591798505   |   Validation Accuracy 0.8912033162777353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:10<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 / 10 :  Loss 0.21863650684618247   |   Validation Accuracy 0.9045673536725325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:10<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 / 10 :  Loss 0.17138371824086468   |   Validation Accuracy 0.9096683423393267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:09<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 / 10 :  Loss 0.14548923287399207   |   Validation Accuracy 0.9110962007459832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:09<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 / 10 :  Loss 0.1275431938343662   |   Validation Accuracy 0.9158661892693191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:09<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 / 10 :  Loss 0.11851181797997609   |   Validation Accuracy 0.9114795033951798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:09<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 / 10 :  Loss 0.11951883757714858   |   Validation Accuracy 0.9094890194625096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:09<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 10 :  Loss 0.11853694210795662   |   Validation Accuracy 0.9124228164451033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_229231/3006362193.py:8: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "train_bahdanau_luong(epochs=10,encoder=enbahdanau,decoder=decbahdanau,train_dataloader=train_data,test_dataloader=test_data,lr=0.002,print_every=1,plot_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain,product,combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'B'), ('A', 'C'), ('A', 'D'), ('B', 'C'), ('B', 'D'), ('C', 'D')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(combinations(\"ABCD\",2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('a', 1), ('b', 4), ('c', 9)], [('a', 1), ('b', 4), ('d', 9)], [('a', 1), ('c', 4), ('b', 9)], [('a', 1), ('c', 4), ('d', 9)], [('a', 1), ('d', 4), ('b', 9)], [('a', 1), ('d', 4), ('c', 9)], [('b', 1), ('a', 4), ('c', 9)], [('b', 1), ('a', 4), ('d', 9)], [('b', 1), ('c', 4), ('a', 9)], [('b', 1), ('c', 4), ('d', 9)], [('b', 1), ('d', 4), ('a', 9)], [('b', 1), ('d', 4), ('c', 9)], [('c', 1), ('a', 4), ('b', 9)], [('c', 1), ('a', 4), ('d', 9)], [('c', 1), ('b', 4), ('a', 9)], [('c', 1), ('b', 4), ('d', 9)], [('c', 1), ('d', 4), ('a', 9)], [('c', 1), ('d', 4), ('b', 9)], [('d', 1), ('a', 4), ('b', 9)], [('d', 1), ('a', 4), ('c', 9)], [('d', 1), ('b', 4), ('a', 9)], [('d', 1), ('b', 4), ('c', 9)], [('d', 1), ('c', 4), ('a', 9)], [('d', 1), ('c', 4), ('b', 9)]]\n"
     ]
    }
   ],
   "source": [
    "# python program to demonstrate\n",
    "# unique combination of two lists\n",
    "# using zip() and permutation of itertools\n",
    " \n",
    "# import itertools package\n",
    "import itertools\n",
    "from itertools import permutations\n",
    " \n",
    "# initialize lists\n",
    "list_1 = [\"a\", \"b\", \"c\",\"d\"]\n",
    "list_2 = [1,4,9]\n",
    "unique_combinations = []\n",
    "permut = itertools.permutations(list_1, len(list_2))\n",
    "\n",
    "for comb in permut:\n",
    "    zipped = zip(comb, list_2)\n",
    "    unique_combinations.append(list(zipped))\n",
    "print(unique_combinations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "\n",
    "config=ConfigParser()\n",
    "\n",
    "config.read('config.ini')\n",
    "\n",
    "\n",
    "encoding_dict={key:int(value) if value.isdigit() else value for key,value in config._sections['model'].items()}\n",
    "encoding_dict['enc_directions']=2 if encoding_dict['enc_bidirection'].lower()=='True' else 1\n",
    "encoding_dict['dec_directions']=2 if encoding_dict['dec_bidirection'].lower()=='True' else 1\n",
    "\n",
    "\n",
    "batch_size=int(config.get(\"training\",\"batch_size\"))\n",
    "test_size=float(config.get('training','test_size'))\n",
    "MAX_LENGTH=int(config.get('training','max_length'))\n",
    "epochs=int(config.get(\"training\",\"epochs\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if (test_size>1 or test_size<0):\n",
    "    raise ValueError(\"test_size must be a ratio\") \n",
    "\n",
    "if config.get(\"model\",\"attention_variant\").lower()=='luong':\n",
    "    type_=config.get(\"Luong\",\"luong_type\")\n",
    "    attention_variant=config.get(\"Luong\",\"luong_variant\")\n",
    "    if type_==\"local\":\n",
    "        d=int(config.get(\"Luong\",\"d\"))\n",
    "        max_L_d=(MAX_LENGTH,d)\n",
    "elif config.get(\"model\",\"attention_variant\").lower()=='bahdanau':\n",
    "    pass\n",
    "else: \n",
    "    raise ValueError(\"Attention variants can either be 'luong' or 'bahdanau'\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.read('config.ini')\n",
    "if config.get(\"model\",\"dec_bidirection\").lower()==\"True\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m config\u001b[39m.\u001b[39m_sections[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "config._sections['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': '', 'max_length': ''}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config._sections['training']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
